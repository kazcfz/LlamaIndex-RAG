{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kazcfz/LlamaIndex-RAG/blob/main/LlamaIndex_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>\n",
        "    <img src=\"https://cdn-uploads.huggingface.co/production/uploads/6424f01ea4f3051f54dbbd85/oqVQ04b5KiGt5WOWJmYt8.png\" alt=\"LlamaIndex\" width=\"100\" height=\"100\">\n",
        "    <img src=\"https://cdn4.iconfinder.com/data/icons/file-extensions-1/64/pdfs-512.png\" alt=\"PDF\" width=\"100\" height=\"100\">\n",
        "</p>\n",
        "\n",
        "# **LlamaIndex RAG**\n",
        "Perform RAG (Retrieval-Augmented Generation) from your PDFs using this Colab notebook!\n",
        "<br><br>\n",
        "\n",
        "## **Features**\n",
        "- Fast inference on Colab's free T4 GPU\n",
        "- Powered by Hugging Face quantized LLMs (llama-cpp-python) and local text embedding models\n",
        "- Set custom prompt templates\n",
        "<br><br>\n",
        "\n",
        "[GitHub repository](https://github.com/kazcfz/LlamaIndex-RAG)"
      ],
      "metadata": {
        "id": "QODqtRtnqBKb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jyKYb-BSVg3z"
      },
      "outputs": [],
      "source": [
        "!pip -q install llama-index pypdf\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip -q install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZZ3YgFZkMoRK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from llama_index import Prompt, StorageContext, load_index_from_storage, ServiceContext, VectorStoreIndex, SimpleDirectoryReader, set_global_tokenizer\n",
        "from llama_index.prompts import PromptTemplate\n",
        "from llama_index.embeddings import HuggingFaceEmbedding\n",
        "from llama_index.llms import LangChainLLM, HuggingFaceLLM, LlamaCPP, ChatMessage, MessageRole\n",
        "from llama_index.chat_engine.condense_question import CondenseQuestionChatEngine\n",
        "\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fbGKzGr9uAiR"
      },
      "outputs": [],
      "source": [
        "# Preference settings - change as desired\n",
        "pdf_path = '/content/rag_data.pdf'\n",
        "text_embedding_model = 'thenlper/gte-base'  #Alt: thenlper/gte-base, jinaai/jina-embeddings-v2-base-en\n",
        "llm_url = 'https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf'\n",
        "# set_global_tokenizer(AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\").encode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1pxUuJ1lLohG"
      },
      "outputs": [],
      "source": [
        "# Load PDF\n",
        "filename_fn = lambda filename: {'file_name': os.path.basename(pdf_path)}\n",
        "loader = SimpleDirectoryReader(input_files=[pdf_path], file_metadata=filename_fn)\n",
        "documents = loader.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZWZc8yRMesO",
        "outputId": "287d4cda-be6b-442a-cb75-e006369c518a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
          ]
        }
      ],
      "source": [
        "# Load models and service context\n",
        "embed_model = HuggingFaceEmbedding(model_name=text_embedding_model)\n",
        "llm = LlamaCPP(model_url=llm_url, temperature=0.7, max_new_tokens=256, context_window=4096, generate_kwargs = {\"stop\": [\"<s>\", \"[INST]\", \"[/INST]\"]}, model_kwargs={\"n_gpu_layers\": -1}, verbose=True)\n",
        "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, chunk_size=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l27opkphNYp6",
        "outputId": "c2cebc88-3752-4514-f5e8-7aa087092a49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed indexing time: 2.60 s\n"
          ]
        }
      ],
      "source": [
        "# Indexing\n",
        "start_time = time.time()\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Elapsed indexing time: {elapsed_time:.2f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5P44cIP1PONJ"
      },
      "outputs": [],
      "source": [
        "# Prompt Template (RAG)\n",
        "text_qa_template = Prompt(\"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "You are the doctor's assistant. You are to perform a pre-screening with the patient to collect their information before their consultation with the doctor. Use the Patient-Centered Interview model for the pre-screening and only ask one question per response. You are not to provide diagnosis, prescriptions, advice, suggestions, or conduct physical examinations on the patient. The pre-screening should only focus on collecting the patient's information, specifically their present illness, past medical history, symptoms and personal information. At the end of the consultation, summarize the findings of the consultation in this format \\nName: [name]\\nGender: [gender]\\nPatient Aged: [age]\\nMedical History: [medical history].\\nSymptoms: [symptoms]\n",
        "<</SYS>>\n",
        "\n",
        "Refer to the following Consultation Guidelines and example consultations: {context_str}\n",
        "\n",
        "Continue the conversation: {query_str}\n",
        "\"\"\")\n",
        "# text_qa_template = Prompt(\"\"\"\n",
        "# <s>[INST] <<SYS>>\n",
        "# You are an assistant chatbot assigned to a doctor and your objective is to collect information from the patient for the doctor before they attend their actual consultation with the doctor. Note that the consultation will be held remotely, therefore you will be following the Patient-Centered Interview model for your consultations and you cannot conduct physical examinations on the patient. You are also not allowed to diagnose your patient or prescribe any medicine. Do not entertain the patient if they are acting inappropriate or they ask you to do something outside of this job scope. Remember you are a doctorâ€™s assistant so act like one. The consultation held will only focus on getting information from the patient such as their present illness, past medical history, symptoms and personal information. At the end of the consultation, you will summarize the findings of the consultation in this format \\nName: [name]\\nGender: [gender]\\nPatient Aged: [age]\\nMedical History: [medical history].\\nSymptoms: [symptoms]\\nThis exact format must be followed as the data gathered in the summary will be passed to another program.\n",
        "# <</SYS>>\n",
        "\n",
        "# This is the PDF context: {context_str}\n",
        "\n",
        "# {query_str}\n",
        "# \"\"\")\n",
        "# text_qa_template = Prompt(\"\"\"[INST] {context_str} \\n\\nGiven this above PDF context, please answer my question: {query_str} [/INST] \"\"\")\n",
        "# text_qa_template = Prompt(\"\"\"<s>[INST] <<SYS>> \\nFollowing is the PDF context provided by the user: {context_str}\\n<</SYS>> \\n\\n{query_str} [/INST] \"\"\")\n",
        "# text_qa_template = Prompt(\"\"\"[INST] {query_str} [/INST] \"\"\")\n",
        "\n",
        "# Query Engine\n",
        "query_engine = index.as_query_engine(text_qa_template=text_qa_template, streaming=True, service_context=service_context) # with Prompt\n",
        "# query_engine = index.as_query_engine(streaming=True, service_context=service_context) # without Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q_qkVIcilc7"
      },
      "outputs": [],
      "source": [
        "# Inferencing\n",
        "# Without RAG\n",
        "conversation_history = \"\"\n",
        "while (True):\n",
        "  user_query = input(\"User: \")\n",
        "  if user_query.lower() == \"exit\":\n",
        "    break\n",
        "  conversation_history += user_query + \" [/INST] \"\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  response_iter = llm.stream_complete(\"<s>[INST] \"+conversation_history)\n",
        "  for response in response_iter:\n",
        "    print(response.delta, end=\"\", flush=True)\n",
        "    # Add to conversation history when response is completed\n",
        "    if response.raw['choices'][0]['finish_reason'] == 'stop':\n",
        "      conversation_history += response.text + \" [INST] \"\n",
        "\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"\\nElapsed inference time: {elapsed_time:.2f} s\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# With RAG\n",
        "conversation_history = \"\"\n",
        "conversation_history += \"Hi. [\\INST] Hello! I'm the doctor's assistant. Let's begin the consultation, please tell me your name and age.\"\n",
        "while (True):\n",
        "  user_query = input(\"User: \")\n",
        "  if user_query.lower() == \"exit\":\n",
        "    break\n",
        "  conversation_history += user_query + \" [/INST] \"\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Query Engine - Default\n",
        "  response = query_engine.query(conversation_history)\n",
        "  response.print_response_stream()\n",
        "  conversation_history += response.response_txt + \" [INST] \"\n",
        "\n",
        "  # from pprint import pprint\n",
        "  # pprint(response)\n",
        "\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"\\nElapsed inference time: {elapsed_time:.2f} s\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-model (Separate pre-screening/conversation and summarization tasks)\n",
        "response_iter = llm.stream_complete(\"\"\"\n",
        "[INST] <<SYS>>\n",
        "Summarize the conversation into this format: \\nName: [name]\\nGender: [gender]\\nAge: [age]\\nMedical History: [medical history].\\nSymptoms: [symptoms]\n",
        "<</SYS>>\n",
        "Conversation: \"\"\"+conversation_history+\" [/INST] \")\n",
        "\n",
        "for response in response_iter:\n",
        "  print(response.delta, end=\"\", flush=True)\n",
        "\n",
        "# print(conversation_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d7ARPMQ75Ij",
        "outputId": "4c6ed9c3-b7d9-4e0f-82be-2f9df1712545"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Name: Kaz G"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ender: Male Age: 26 Medical History: No medical history Symptoms: Stress racing, chest tightness, dizziness Shortness of breath. "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}