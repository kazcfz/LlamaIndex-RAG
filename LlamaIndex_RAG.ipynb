{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kazcfz/LlamaIndex-RAG/blob/main/LlamaIndex_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>\n",
        "    <img src=\"https://cdn-uploads.huggingface.co/production/uploads/6424f01ea4f3051f54dbbd85/oqVQ04b5KiGt5WOWJmYt8.png\" alt=\"LlamaIndex\" width=\"100\" height=\"100\">\n",
        "    <img src=\"https://cdn4.iconfinder.com/data/icons/file-extensions-1/64/pdfs-512.png\" alt=\"PDF\" width=\"100\" height=\"100\">\n",
        "</p>\n",
        "\n",
        "# **LlamaIndex RAG**\n",
        "Perform RAG (Retrieval-Augmented Generation) from your PDFs using this Colab notebook!\n",
        "<br><br>\n",
        "\n",
        "## **Features**\n",
        "- Fast inference on Colab's free T4 GPU\n",
        "- Powered by Hugging Face quantized LLMs (llama-cpp-python) and local text embedding models\n",
        "- Set custom prompt templates\n",
        "<br><br>\n",
        "\n",
        "[GitHub repository](https://github.com/kazcfz/LlamaIndex-RAG)"
      ],
      "metadata": {
        "id": "QODqtRtnqBKb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jyKYb-BSVg3z"
      },
      "outputs": [],
      "source": [
        "!pip -q install llama-index pypdf\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip -q install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZZ3YgFZkMoRK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from llama_index import Prompt, StorageContext, load_index_from_storage, ServiceContext, VectorStoreIndex, SimpleDirectoryReader, set_global_tokenizer\n",
        "from llama_index.prompts import PromptTemplate\n",
        "from llama_index.embeddings import HuggingFaceEmbedding\n",
        "from llama_index.llms import LangChainLLM, HuggingFaceLLM, LlamaCPP, ChatMessage, MessageRole\n",
        "from llama_index.chat_engine.condense_question import CondenseQuestionChatEngine\n",
        "\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fbGKzGr9uAiR"
      },
      "outputs": [],
      "source": [
        "# Preference settings - change as desired\n",
        "pdf_path = '/content/rag_data.pdf'\n",
        "text_embedding_model = 'thenlper/gte-base'  #Alt: thenlper/gte-base, jinaai/jina-embeddings-v2-base-en\n",
        "llm_url = 'https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf'\n",
        "# set_global_tokenizer(AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\").encode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1pxUuJ1lLohG"
      },
      "outputs": [],
      "source": [
        "# Load PDF\n",
        "filename_fn = lambda filename: {'file_name': os.path.basename(pdf_path)}\n",
        "loader = SimpleDirectoryReader(input_files=[pdf_path], file_metadata=filename_fn)\n",
        "documents = loader.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZWZc8yRMesO",
        "outputId": "544e1f87-5432-4ff3-cfe9-874b41430f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
          ]
        }
      ],
      "source": [
        "# Load models and service context\n",
        "embed_model = HuggingFaceEmbedding(model_name=text_embedding_model)\n",
        "llm = LlamaCPP(model_url=llm_url, temperature=0.7, max_new_tokens=256, context_window=4096, generate_kwargs={}, model_kwargs={\"n_gpu_layers\": -1}, verbose=True)\n",
        "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, chunk_size=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l27opkphNYp6",
        "outputId": "11e73994-aef3-467f-f6ed-b33eea8cb6b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Elapsed indexing time: 3.83 s\n"
          ]
        }
      ],
      "source": [
        "# Indexing\n",
        "start_time = time.time()\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Elapsed indexing time: {elapsed_time:.2f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5P44cIP1PONJ"
      },
      "outputs": [],
      "source": [
        "# Prompt Template (RAG)\n",
        "text_qa_template = Prompt(\"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "You are an assistant chatbot assigned to a doctor and your objective is to collect information from the patient for the doctor before they attend their actual consultation with the doctor. Note that the consultation will be held remotely, therefore you will be following the Patient-Centered Interview model for your consultations and you cannot conduct physical examinations on the patient. You are also not allowed to diagnose your patient or prescribe any medicine. Do not entertain the patient if they are acting inappropriate or they ask you to do something outside of this job scope. Remember you are a doctorâ€™s assistant so act like one. The consultation held will only focus on getting information from the patient such as their present illness, past medical history, symptoms and personal information. At the end of the consultation, you will summarize the findings of the consultation in this format \\nName: [name]\\nGender: [gender]\\nPatient Aged: [age]\\nMedical History: [medical history].\\nSymptoms: [symptoms]\\nThis exact format must be followed as the data gathered in the summary will be passed to another program.\n",
        "<</SYS>>\n",
        "\n",
        "This is the PDF context: {context_str}\n",
        "\n",
        "{query_str}\n",
        "\"\"\")\n",
        "# text_qa_template = Prompt(\"\"\"[INST] {context_str} \\n\\nGiven this above PDF context, please answer my question: {query_str} [/INST] \"\"\")\n",
        "# text_qa_template = Prompt(\"\"\"<s>[INST] <<SYS>> \\nFollowing is the PDF context provided by the user: {context_str}\\n<</SYS>> \\n\\n{query_str} [/INST] \"\"\")\n",
        "# text_qa_template = Prompt(\"\"\"[INST] {query_str} [/INST] \"\"\")\n",
        "\n",
        "# Query Engine\n",
        "query_engine = index.as_query_engine(text_qa_template=text_qa_template, streaming=True, service_context=service_context) # with Prompt\n",
        "# query_engine = index.as_query_engine(streaming=True, service_context=service_context) # without Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q_qkVIcilc7",
        "outputId": "7223b028-20bf-4b9d-ed60-868fd3f52f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What does the iPhone feature?\n",
            " The iPhone is a smart"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "phone that features a wide range of hardware and software capabilities. Here are some of the key features that an iPhone typically includes:\n",
            "1. Touchscreen display: The iPhone has a high-resolution touchscreen display that allows you to interact with the device using multi-touch gestures, such as tapping, swiping, pinching, and scrolling.\n",
            "2. Processor: The iPhone is powered by Apple's A-series processor, which provides fast performance and efficient battery life.\n",
            "3. Memory: iPhones typically come with 64GB, 128GB, or 256GB of internal storage, depending on the model. You can also expand the storage capacity using external storage devices like SD cards.\n",
            "4. Camera: The iPhone has a high-quality camera that takes photos and videos with excellent color reproduction and detail. The camera app includes features such as portrait mode, night mode, and video recording in 4K resolution.\n",
            "5. FaceTime HD camera: The front-facing camera on the iPhone is designed for video calls using Apple's FaceTime service, which allows you to make and receive high-quality video calls with other iPhone users.\n",
            "\n",
            "\n",
            "Elapsed inference time: 7.98 s\n",
            "User: Who made it?\n",
            " The"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " iPhone is a line of smartphones designed and marketed by Apple Inc. was first announced in 2007, and has since become one of the most popular smartphones on the market. Here are some of the key features that the iPhone typically includes:\n",
            "1. Touchscreen display: The iPhone features a high-resolution touchscreen display with multi-touch capabilities, allowing users to interact with the device by tapping, swiping, and pinching their fingers on the screen.\n",
            "2. Apple AX processor: The iPhone is powered by Apple's proprietary AX processor, which provides fast performance and efficient battery life.\n",
            "3. iOS operating system: The iPhone runs on Apple's proprietary iOS operating system, which provides a user-friendly interface and access to the App Store, where users can download thousands of third-party apps.\n",
            "4. Camera: The iPhone typically includes a high-resolution rear camera with features such as optical zoom, portrait mode, and night mode, as well as a front-facing camera for selfies and video calls.\n",
            "5. Wi-Fi and cellular connectivity: The iPhone can connect to the internet via Wi-Fi or cellular networks\n",
            "\n",
            "Elapsed inference time: 8.10 s\n",
            "User: exit\n",
            " The iPhone"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " is a line of smartphones designed and marketed by Apple Inc. It was first released in 2007 and has since become one of the most popular smartphones on the market. Here are some of the key features that the iPhone typically includes:\n",
            "1. Touchscreen display: The iPhone features a multi-touch touchscreen display, which allows users to interact with the device by tapping, swiping, and pinching their fingers on the screen.\n",
            "2. iOS operating system: The iPhone runs on Apple's proprietary iOS operating system, which provides a user-friendly interface and access to a wide range of apps from the App Store.\n",
            "3. Camera: The iPhone typically includes a rear-facing camera with advanced features such as optical image stabilization, portrait mode, and night mode. There may also be a front-facing camera for selfies and FaceTime calls.\n",
            "4. Processor: The iPhone is powered by Apple's A-series processor, which provides fast performance and efficient battery life.\n",
            "5. Storage: The iPhone comes in a range of storage sizes, from 64GB to 512GB, depending on the model.\n",
            "6. Wi\n",
            "\n",
            "Elapsed inference time: 8.28 s\n"
          ]
        }
      ],
      "source": [
        "# Inferencing\n",
        "conversation_history = \"\"\n",
        "\n",
        "\n",
        "# Without RAG\n",
        "user_query = \"\"\n",
        "while (user_query != \"exit\"):\n",
        "  user_query = input(\"User: \")\n",
        "  conversation_history += user_query + \" [/INST] \"\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  response_iter = llm.stream_complete(\"<s>[INST] \"+conversation_history)\n",
        "  for response in response_iter:\n",
        "    print(response.delta, end=\"\", flush=True)\n",
        "    # Add to conversation history when response is completed\n",
        "    if response.raw['choices'][0]['finish_reason'] == 'stop':\n",
        "      conversation_history += response.text + \" [INST] \"\n",
        "\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"\\nElapsed inference time: {elapsed_time:.2f} s\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# With RAG\n",
        "user_query = \"\"\n",
        "while (user_query != \"exit\"):\n",
        "  user_query = input(\"User: \")\n",
        "  conversation_history += user_query + \" [/INST] \"\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Query Engine - Default\n",
        "  response = query_engine.query(conversation_history)\n",
        "  response.print_response_stream()\n",
        "  conversation_history += response.response_txt + \" [INST] \"\n",
        "\n",
        "  # from pprint import pprint\n",
        "  # pprint(response)\n",
        "\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"\\nElapsed inference time: {elapsed_time:.2f} s\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}