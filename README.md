<p align="center">
    <img src="https://cdn-uploads.huggingface.co/production/uploads/6424f01ea4f3051f54dbbd85/oqVQ04b5KiGt5WOWJmYt8.png" alt="LlamaIndex" width="12%" height="12%">
    <img src="https://cdn4.iconfinder.com/data/icons/file-extensions-1/64/pdfs-512.png" alt="PDF" width="12%" height="12%">
</p>

# LlamaIndex-RAG
Perform RAG (Retrieval-Augmented Generation) from your PDFs using this Colab notebook!<br>
Powered by Llama 2
<br><br>

## Features
- Free, no API or Token required
- Fast inference on Colab's free T4 GPU
- Powered by Hugging Face quantized LLMs (llama-cpp-python)
- Powered by Hugging Face local text embedding models
- Set custom prompt templates
- Prepared Chat mode (not QA)
<br><br>

## Getting started
1. [Open in colab](https://colab.research.google.com/github/kazcfz/LlamaIndex-RAG/blob/main/LlamaIndex_RAG.ipynb)
2. Make sure the Colab's Runtime Type is set to T4 GPU (at least)
3. Edit preferences in Block 4
4. Upload your PDF into Files (Default name: `rag_data.pdf`)
5. Runtime > Run all
